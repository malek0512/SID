---
title: "Partie 2"
output: html_document
---

*Partie 2 du TP*

Regression logistique

```{r}
Hw <- function(X, w, w0) {return (X %*% w + w0) }
Rm <- function(X, Y, w, w0) { return (mean(log2( 1 + exp( -Y * Hw(X, w, w0)))))}
Gradient <- function(X, Y, w, w0) {return (-mean(Y * (1 - (1/(1+exp(-Y*Hw(X, w, w0)))) ) * X ))}

#Rm(df$X, df$Y, abs(df$Y)[1:9], 0)
regression_logistique <- function(T = 5000, eta = 1.0, precision = 0.1, X, Y) {
  m <- nrow(X)
  d <- ncol(X)
  # Initialisation of the weight vector
  w0 <- 0
  #w <- array(data = 0, dim = d,dimnames = NULL)
  w <- runif(n = d, min = 0, max = d)
  t <- 0
  Rold <- 1
  Rnew <- 0
  
  # While the maximum number of iterations is not reached do 
  while ( abs( Rnew - Rold ) > precision )
  {
    Rold <- Rm(X = X, Y = Y, w = w, w0 = w0)
    
    gradient <- Gradient(X, Y, w, w0)
    w <- w - eta * gradient * Rm(X = X, Y = Y, w = w, w0 = w0)
    w0 <- w0 - eta * 
    Rnew <- Rm(X = X, Y = Y, w = w, w0 = w0)
      
    t <- t + 1
  }
  
  return (list(w0=w0, w=w))
}
```


```{r}
# Adaboost
adaboost <- function(T = 5000, X, Y){
  m <- length(Y)
  D <- array(data = 1/m, dim = d,dimnames = NULL)
  df <- data.frame()
  
  for (i in 1:T) 
  {
    #apprendre un classifieur faible en utilisant Dt
    hw <- function(X, w, w0) {return (X %*% w + w0) }
    
    errort <- 0
    for (i in 1:m) 
      error <- error + (if ( X[i,] %*% w + w0 != Y[i] ) D[i] else 0)
    
    alphat <- log2((1-errort)/errort) / 2
    
    #maj des poids
    Zt <- sum(D * exp(-alphat * Y * hw(X)))
    D <- (D * exp(-alphat * Y * hw(X))) / Zt

    equation <- list(w = w, w0 = w0)
    df = rbind(df, data.frame(alpha = alphat, equation = equation))
  }
  #Hw <- 
}

```


